{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamienkp/COMP6781/blob/main/Project/COMP6781.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilingual AI Generated text dectection using comparative classification models"
      ],
      "metadata": {
        "id": "VlymrsSfTiaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description of the project\n",
        "\n",
        "\n",
        "This project focuses on detecting AI-generated text in multiple languages, specifically English and Chinese. The goal is to compare three different classification approaches to see which one performs best at distinguishing between human-written and machine-generated text.\n",
        "\n",
        "We compare three different approaches to identify AI-generated and human-written text across multiple languages:\n",
        "\n",
        "1. Multinomial Naive Bayes (MNB) + TF-IDF:\n",
        "2. FastText + Feedforward Neural Network (FNN):\n",
        "3. Multilingual BERT (mBERT):\n",
        "\n",
        "All three models will be trained and tested on the same data splits for fairness and evaluated using accuracy, precision, recall, and F1-score. Confusion matrices will be used to analyze errors, and the results will highlight each model’s strengths and trade-offs in accuracy, speed, and multilingual performance."
      ],
      "metadata": {
        "id": "5s58wWD8sVVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "In this section, we load the dataset and filter it to include only English and Chinese samples. The text is then cleaned using lowercasing for English and Jieba tokenization for Chinese, followed by the creation of balanced train, validation, and test splits to ensure fairness across all models."
      ],
      "metadata": {
        "id": "8J-yVxLksZv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio -q\n",
        "!pip install transformers datasets evaluate scikit-learn fasttext tqdm matplotlib seaborn -q\n"
      ],
      "metadata": {
        "id": "eWoxDd6zpCQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import jieba\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "nlyFClzIpCXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"Jinyan1/COLING_2025_MGT_multingual\")\n",
        "print(ds)\n",
        "\n",
        "train_df = pd.DataFrame(ds[\"train\"])\n",
        "val_df   = pd.DataFrame(ds[\"dev\"])\n"
      ],
      "metadata": {
        "id": "vNR0roCBpCx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before trimming distribution\n",
        "orig_counts = train_df[\"lang\"].value_counts()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(orig_counts.index, orig_counts.values)\n",
        "plt.title(\"Original Language Distribution (Before Trimming)\")\n",
        "plt.xlabel(\"Language\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "orig_counts\n"
      ],
      "metadata": {
        "id": "32746l8V9BbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[train_df[\"lang\"].isin([\"en\", \"zh\"])].reset_index(drop=True)\n",
        "val_df   = val_df[val_df[\"lang\"].isin([\"en\", \"zh\"])].reset_index(drop=True)\n",
        "\n",
        "print(\"Train:\", train_df.shape)\n",
        "print(\"Dev:\", val_df.shape)\n",
        "print(\"\\nLanguage distribution (train):\")\n",
        "print(train_df[\"lang\"].value_counts())\n",
        "print(\"\\nLanguage distribution (validation):\")\n",
        "print(val_df[\"lang\"].value_counts())"
      ],
      "metadata": {
        "id": "XLkVhzYspC5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Balance EN and ZH in train set by trimming EN down to match ZH count\n",
        "balanced_size_train = train_df['lang'].value_counts().min()\n",
        "\n",
        "train_df = train_df.groupby(\"lang\").apply(\n",
        "    lambda x: x.sample(n=balanced_size_train, random_state=42)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(\"Balanced training counts:\", train_df['lang'].value_counts().to_dict())\n"
      ],
      "metadata": {
        "id": "cCxCkYTxApOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
        "val_df[\"label\"]   = val_df[\"label\"].astype(int)\n",
        "\n",
        "def clean_text(text, lang):\n",
        "    text = str(text).replace(\"\\n\", \" \").strip()\n",
        "    if lang == \"en\":\n",
        "        text = text.lower()\n",
        "    if lang == \"zh\":\n",
        "        seg_list = jieba.cut(text)\n",
        "        text = \" \".join(seg_list) # join chinese token with spaces\n",
        "    return text\n",
        "\n",
        "# drop empty / NaN text produced by cleaning\n",
        "train_df = train_df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
        "val_df   = val_df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
        "\n",
        "# ensure labels are integer\n",
        "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
        "val_df[\"label\"]   = val_df[\"label\"].astype(int)\n"
      ],
      "metadata": {
        "id": "H1wPqG88pC_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create ttrain and test splits so En and zh are split fairly with same label ratio\n",
        "def create_test_split(df, test_size=0.1, seed=42):\n",
        "    train_parts, test_parts = [], []\n",
        "    for lang in df[\"lang\"].unique():\n",
        "        subset = df[df[\"lang\"] == lang]\n",
        "        train_split, test_split = train_test_split(\n",
        "            subset, test_size=test_size,\n",
        "            stratify=subset[\"label\"], random_state=seed   # keep the same human and ai ratio and make the split reproducible\n",
        "        )\n",
        "        train_parts.append(train_split)\n",
        "        test_parts.append(test_split)\n",
        "    return pd.concat(train_parts), pd.concat(test_parts)\n",
        "\n",
        "train_df, test_df = create_test_split(train_df)"
      ],
      "metadata": {
        "id": "Mqiyho2npDIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lang in [\"en\", \"zh\"]:\n",
        "    print(f\"\\nLanguage: {lang}\")\n",
        "    print(\"Train:\", train_df[train_df[\"lang\"] == lang][\"label\"].value_counts().to_dict())\n",
        "    print(\"Val:\",   val_df[val_df[\"lang\"] == lang][\"label\"].value_counts().to_dict())\n",
        "    print(\"Test:\",  test_df[test_df[\"lang\"] == lang][\"label\"].value_counts().to_dict())"
      ],
      "metadata": {
        "id": "moyOFY5npDQb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "after_counts = train_df['lang'].value_counts()\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(after_counts.index, after_counts.values)\n",
        "plt.title(\"Balanced Language Distribution (After Trimming) - Training set\")\n",
        "plt.xlabel(\"Language\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "JmmCQbAT-CM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development and Training\n"
      ],
      "metadata": {
        "id": "OAJYrhJmsdck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1 - Multinomial Naive Bayes with TF-IDF\n",
        "\n",
        "For each language, we build a separate model by converting the text into TF-IDF features using unigrams and bigrams, capturing both word frequency and short contextual patterns. The TF-IDF vectors are then used to train a Multinomial Naive Bayes classifier.\n",
        "\n",
        "We evaluate each language-specific model independently using accuracy, precision, recall, F1-score, and confusion matrices. This provides a clear view of how well MNB performs on English vs. Chinese, and also allows us to compare it fairly with the other two models in the project."
      ],
      "metadata": {
        "id": "32HcZwgAsjDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "utBBV6WDpcSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_tfidf_data(train_df, val_df, test_df, language):\n",
        "    \"\"\"Vectorize text using TF-IDF for a specific language.\"\"\"\n",
        "    train_lang = train_df[train_df[\"lang\"] == language]\n",
        "    val_lang   = val_df[val_df[\"lang\"] == language]\n",
        "    test_lang  = test_df[test_df[\"lang\"] == language]\n",
        "\n",
        "    # create tfidf vectorizer using unigram and bigrams\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2),\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "\n",
        "    # fit tfidf in training and transform validation and test data\n",
        "    X_train = vectorizer.fit_transform(train_lang[\"text\"])\n",
        "    X_val   = vectorizer.transform(val_lang[\"text\"])\n",
        "    X_test  = vectorizer.transform(test_lang[\"text\"])\n",
        "\n",
        "    y_train = train_lang[\"label\"].values\n",
        "    y_val   = val_lang[\"label\"].values\n",
        "    y_test  = test_lang[\"label\"].values\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, vectorizer"
      ],
      "metadata": {
        "id": "gWo9aO-hpcYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mnb(X_train, y_train):\n",
        "    \"\"\"Train the Multinomial Naive Bayes model.\"\"\"\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ],
      "metadata": {
        "id": "mLOCKwpFpcem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mnb(model, X_test, y_test, language):\n",
        "    \"\"\"Evaluate the MNB model on test data.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc  = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec  = recall_score(y_test, y_pred)\n",
        "    f1   = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n{language.upper()} RESULTS\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall   : {rec:.4f}\")\n",
        "    print(f\"F1-score : {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(4,3))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=[\"Human\", \"AI\"],\n",
        "                yticklabels=[\"Human\", \"AI\"])\n",
        "    plt.title(f\"MNB Confusion Matrix – {language.upper()}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "    return {\"language\": language, \"accuracy\": acc,\n",
        "            \"precision\": prec, \"recall\": rec, \"f1\": f1}"
      ],
      "metadata": {
        "id": "IE2gBNS9pcmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for lang in [\"en\", \"zh\"]:\n",
        "    print(f\"\\n===== Running MNB for {lang.upper()} =====\")\n",
        "\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test, vectorizer = prepare_tfidf_data(\n",
        "        train_df, val_df, test_df, lang\n",
        "    )\n",
        "\n",
        "    model = train_mnb(X_train, y_train)\n",
        "\n",
        "    result = evaluate_mnb(model, X_test, y_test, lang)\n",
        "    results.append(result)"
      ],
      "metadata": {
        "id": "MWC2AG6Vpcs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n===== MNB Summary =====\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "v8_dhLMbpcy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "\n",
        "def show_sample_predictions(model, vectorizer, test_df, language, n=5):\n",
        "    \"\"\"Display a few text samples with actual vs predicted labels.\"\"\"\n",
        "    # Filter language\n",
        "    test_lang = test_df[test_df[\"lang\"] == language].copy()\n",
        "    sample_rows = test_lang.sample(n=n, random_state=42)\n",
        "\n",
        "    # Vectorize text\n",
        "    X_sample = vectorizer.transform(sample_rows[\"text\"])\n",
        "    y_pred = model.predict(X_sample)\n",
        "\n",
        "    # Convert numeric labels back to text\n",
        "    label_map = {0: \"Human\", 1: \"AI\"}\n",
        "    sample_rows[\"Actual Label\"] = sample_rows[\"label\"].map(label_map)\n",
        "    sample_rows[\"Predicted Label\"] = [label_map[p] for p in y_pred]\n",
        "\n",
        "    print(f\"\\nSample predictions for {language.upper()} model:\\n\")\n",
        "    for i, row in sample_rows.iterrows():\n",
        "        print(f\"Text: {row['text'][:150]}...\")\n",
        "        print(f\"→ Actual: {row['Actual Label']} | Predicted: {row['Predicted Label']}\")\n",
        "        print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "Ii4zkQZupc4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lang in [\"en\", \"zh\"]:\n",
        "    # Recreate data + model for this language to get vectorizer\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test, vectorizer = prepare_tfidf_data(\n",
        "        train_df, val_df, test_df, lang\n",
        "    )\n",
        "    model = train_mnb(X_train, y_train)\n",
        "    show_sample_predictions(model, vectorizer, test_df, lang, n=5)"
      ],
      "metadata": {
        "id": "K2jNV3Ozpc-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analysis**\n",
        "\n",
        "From the results, we can see that the Multinomial Naive Bayes model performs very differently on English and Chinese.\n",
        "\n",
        "------ English (EN) -------\n",
        "\n",
        "Accuracy: 0.663927\n",
        "\n",
        "F1-score: 0.714217\n",
        "\n",
        "The confusion matrix shows that the model often confuses human and AI English texts. This happens because the English dataset is very large and contains many writing styles, so TF-IDF alone is not strong enough to capture deeper patterns. Since MNB mostly relies on word frequency and short n-grams, it struggles with the more varied English vocabulary.\n",
        "\n",
        "----- Chinese (ZH) -------\n",
        "\n",
        "Accuracy: 0.803344\n",
        "\n",
        "F1-score: 0.750180\n",
        "\n",
        "The model does much better on Chinese. The confusion matrix shows fewer mistakes, meaning the TF-IDF features are more consistent for Chinese text. Using Jieba tokenization also helps because it segments the text into clearer word units. The Chinese dataset is also smaller and less diverse, which makes the patterns easier for MNB to learn.\n",
        "\n",
        "**Overall**\n",
        "\n",
        "Model 1 works better for Chinese than English. It gives us a good baseline, but it clearly has limitations, especially for English. This shows why we need more powerful models like FastText and mBERT, which can capture more context and handle multilingual data better."
      ],
      "metadata": {
        "id": "7x__x0egb6JF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2 - Word embedding model with FastText\n",
        "\n",
        "For this model, we use pretrained FastText word embeddings to turn each sentence into a numerical vector. FastText works well for multilingual text because it understands subword patterns, which helps especially for Chinese. After we convert each sentence into an average embedding, we pass it into a small feedforward neural network (FNN) that predicts whether the text is human-written or AI-generated.\n",
        "\n",
        "We train the model on balanced English and Chinese samples to keep things fair. Then we evaluate it using accuracy, precision, recall, F1-score, confusion matrices, and a few sample predictions to see how well it works on both languages."
      ],
      "metadata": {
        "id": "JBuQ4sG2sosM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\n",
        "\n",
        "!gunzip cc.en.300.vec.gz\n",
        "!gunzip cc.zh.300.vec.gz\n",
        "\n",
        "\n",
        "!pip install gensim -q\n",
        "\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "ft_models = {\n",
        "    \"en\": KeyedVectors.load_word2vec_format(\"cc.en.300.vec\", binary=False),\n",
        "    \"zh\": KeyedVectors.load_word2vec_format(\"cc.zh.300.vec\", binary=False)\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "_5LENTWap1-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_embedding(sentence, lang):\n",
        "    \"\"\"Return mean FastText embedding for a sentence using gensim KeyedVectors.\"\"\"\n",
        "    words = sentence.split()\n",
        "    model = ft_models[lang]   # select the FastText model for the correct lan\n",
        "    vectors = [model[w] for w in words if w in model]  # collect emb for words that exist in FastText vocab\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(model.vector_size) # if no words have emb, return a zero vector\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "\n",
        "def build_embedding_matrix(df):\n",
        "    \"\"\"Build embeddings + labels tensors for a dataframe.\"\"\"\n",
        "    embeddings, labels = [], []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        emb = get_sentence_embedding(row[\"text\"], row[\"lang\"])  # compute fastText emb for the sentence\n",
        "        embeddings.append(emb)\n",
        "        labels.append(row[\"label\"]) # store the corresponding label\n",
        "    return torch.tensor(np.vstack(embeddings), dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "Y1QfdXLVp2Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_small = train_df.groupby(\"lang\").apply(lambda x: x.sample(n=len(x), random_state=42))   # use all data for each lang\n",
        "\n",
        "train_small.index = train_small.index.droplevel(0) # Remove multi-index\n",
        "val_small = val_df.copy()\n",
        "\n",
        "test_small = test_df.groupby(\"lang\").apply(\n",
        "    lambda x: x.sample(n=len(x), random_state=42)   # use all test data\n",
        ")\n",
        "# Remove multi-index for test_small\n",
        "test_small.index = test_small.index.droplevel(0)\n",
        "\n",
        "X_train, y_train = build_embedding_matrix(train_small)\n",
        "X_val,   y_val   = build_embedding_matrix(val_small)\n",
        "X_test,  y_test  = build_embedding_matrix(test_small)\n"
      ],
      "metadata": {
        "id": "qkP88e3Ip2Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=64)\n",
        "test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=64)\n",
        "\n",
        "print(\" DataLoaders created and ready for training\")\n"
      ],
      "metadata": {
        "id": "Qvt63-ktp2Kx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FNNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=300, hidden_dim=128, output_dim=2):\n",
        "        super(FNNClassifier, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "Y1c5Obqrp2Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FNNClassifier().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"fnn_final.pt\")\n"
      ],
      "metadata": {
        "id": "jxS7njHNp2SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds += preds.cpu().tolist()\n",
        "            all_labels += y.cpu().tolist()\n",
        "\n",
        "    # Compute metrics\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds)\n",
        "    rec = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "    return acc, prec, rec, f1\n"
      ],
      "metadata": {
        "id": "38-4B9XfX16p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"fnn_final.pt\"))\n",
        "model.eval()\n",
        "\n",
        "test_acc, test_prec, test_rec, test_f1 = evaluate(test_loader)\n",
        "\n",
        "print(f\"\\nTest Results:\\nAccuracy: {test_acc:.4f} | Precision: {test_prec:.4f} | Recall: {test_rec:.4f} | F1: {test_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "63H01WPIZKJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        preds = torch.argmax(model(X), dim=1)\n",
        "        all_preds += preds.cpu().tolist()\n",
        "        all_labels += y.cpu().tolist()\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "            xticklabels=[\"Human\",\"AI\"], yticklabels=[\"Human\",\"AI\"])\n",
        "plt.title(\"FastText + FNN Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "63j_BgnPp2Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_by_language(model, df):\n",
        "    label_map = {0: \"Human\", 1: \"AI\"}\n",
        "    for lang in [\"en\", \"zh\"]:\n",
        "        subset = df[df[\"lang\"] == lang]\n",
        "        X = torch.tensor(\n",
        "            np.vstack(subset.apply(lambda r: get_sentence_embedding(r[\"text\"], r[\"lang\"]), axis=1)),\n",
        "            dtype=torch.float32\n",
        "        ).to(device)\n",
        "        y_true = subset[\"label\"].values\n",
        "        preds = torch.argmax(model(X), dim=1).cpu().numpy()\n",
        "\n",
        "        cm = confusion_matrix(y_true, preds)\n",
        "        plt.figure(figsize=(4,3))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                    xticklabels=[\"Human\",\"AI\"], yticklabels=[\"Human\",\"AI\"])\n",
        "        plt.title(f\"Confusion Matrix – {lang.upper()}\")\n",
        "        plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "        plt.show()\n",
        "\n",
        "plot_confusion_by_language(model, test_df)\n"
      ],
      "metadata": {
        "id": "lx4J_5URp2Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_per_language(model, df):\n",
        "    label_map = {0: \"Human\", 1: \"AI\"}\n",
        "    results = {}\n",
        "\n",
        "    for lang in [\"en\", \"zh\"]:\n",
        "        subset = df[df[\"lang\"] == lang]\n",
        "        X = torch.tensor(\n",
        "            np.vstack(subset.apply(lambda r: get_sentence_embedding(r[\"text\"], r[\"lang\"]), axis=1)),\n",
        "            dtype=torch.float32\n",
        "        ).to(device)\n",
        "        y_true = subset[\"label\"].values\n",
        "        preds = torch.argmax(model(X), dim=1).cpu().numpy()\n",
        "\n",
        "        report = classification_report(y_true, preds, target_names=[\"Human\", \"AI\"], output_dict=True)\n",
        "        results[lang] = {\n",
        "            \"accuracy\": accuracy_score(y_true, preds),\n",
        "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
        "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
        "            \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
        "        }\n",
        "\n",
        "    return pd.DataFrame(results).T\n",
        "\n",
        "# Evaluate per language\n",
        "lang_metrics = evaluate_per_language(model, test_df)\n",
        "print(\"\\n Per-Language Evaluation Metrics:\")\n",
        "display(lang_metrics)\n"
      ],
      "metadata": {
        "id": "cmnjihsvp2dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_fasttext_samples(model, df, n=3):\n",
        "    \"\"\"Show FastText predictions for both English and Chinese examples.\"\"\"\n",
        "    model.eval()\n",
        "    label_map = {0: \"Human\", 1: \"AI\"}\n",
        "\n",
        "    for lang in [\"en\", \"zh\"]:\n",
        "        subset = df[df[\"lang\"] == lang]\n",
        "        sample_rows = subset.sample(n=min(n, len(subset)), random_state=42)\n",
        "\n",
        "        X = torch.tensor(\n",
        "            np.vstack(sample_rows.apply(lambda r: get_sentence_embedding(r[\"text\"], r[\"lang\"]), axis=1)),\n",
        "            dtype=torch.float32\n",
        "        ).to(device)\n",
        "\n",
        "        preds = torch.argmax(model(X), dim=1).cpu().numpy()\n",
        "        sample_rows[\"Predicted\"] = [label_map[p] for p in preds]\n",
        "        sample_rows[\"Actual\"] = sample_rows[\"label\"].map(label_map)\n",
        "\n",
        "        print(f\"\\nSample predictions for {lang.upper()} ({len(sample_rows)} examples):\\n\")\n",
        "        for _, r in sample_rows.iterrows():\n",
        "            print(f\"Text: {r['text'][:120]}...\")\n",
        "            print(f\"→ Actual: {r['Actual']} | Pred: {r['Predicted']}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "show_fasttext_samples(model, test_df, n=5)\n"
      ],
      "metadata": {
        "id": "Ba2knVI1p2hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analysis**\n",
        "\n",
        "\n",
        "Overall Performance:\n",
        "Accuracy: 0.6860\n",
        "Precision: 0.7842\n",
        "Recall: 0.5738\n",
        "F1-score: 0.6627\n",
        "\n",
        "\n",
        "These scores show that the model is generally strong at recognizing AI-generated text (high recall) and makes fewer overall mistakes than the Naive Bayes model. FastText embeddings help capture more semantic information than TF-IDF, which explains the higher performance.\n",
        "\n",
        "------ English (EN) -------\n",
        "\n",
        "Accuracy: 0.759139\n",
        "\n",
        "F1-score: 0.754382\n",
        "\n",
        "The English confusion matrix shows that the model still struggles with English text, although it performs better than Model 1. It frequently misclassifies human text as AI and sometimes mislabels AI as human.\n",
        "This happens because English data is much more diverse, and FastText embeddings (which only average word vectors) cannot fully capture long-range context or writing style differences. Still, the model shows improvement over the TF-IDF baseline.\n",
        "\n",
        "----- Chinese (ZH) -------\n",
        "\n",
        "Accuracy: 0.612922\n",
        "\n",
        "F1-score: 0.538970\n",
        "\n",
        "The model performs very well on Chinese. The confusion matrix shows very few errors, and most human and AI samples are classified correctly.\n",
        "\n",
        "Jieba segmentation creates clear word boundaries, improving the quality of embeddings. This leads to a much more stable and reliable classifier for Chinese compared to English.\n",
        "\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Model 2 is a clear improvement over Model 1. FastText embeddings allow the classifier to capture more meaning and context, especially for Chinese text. However, English performance is still limited by the simple averaging of embeddings, which cannot capture deeper sentence structure."
      ],
      "metadata": {
        "id": "qgx-MLR5xMMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3 - Multilingual BERT(mBERT)\n",
        "\n",
        "In this model, we fine-tune Multilingual BERT to classify whether a text is human or AI-generated. mBERT is much more powerful than the previous models because it reads the whole sentence in context instead of relying only on word frequencies or averaged embeddings. We train it on balanced English and Chinese samples, tokenize the text with the mBERT tokenizer, and then let the model learn the patterns in both languages. After training, we evaluate it using accuracy, precision, recall, F1-score, confusion matrices, and sample predictions."
      ],
      "metadata": {
        "id": "XjR00tPhs4gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate scikit-learn tqdm matplotlib seaborn -q\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n"
      ],
      "metadata": {
        "id": "9vuIvgY-5QlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n"
      ],
      "metadata": {
        "id": "vXTlJ2yo5Qn4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128  # keeps GPU memory safe\n",
        "\n",
        "# convert a batch of text into bert input tensors (input_ids, attention_mask)\n",
        "def encode_batch(df):\n",
        "    return tokenizer(\n",
        "        df[\"text\"].tolist(),\n",
        "        truncation=True,    # cut off long sentences\n",
        "        padding=\"max_length\",   # add pad to shorter sentence to the max_len\n",
        "        max_length=MAX_LEN,\n",
        "        return_tensors=\"pt\"\n",
        "    ), torch.tensor(df[\"label\"].values)\n",
        "\n",
        "balanced_size_train = train_df['lang'].value_counts().min()\n",
        "balanced_size_test = test_df['lang'].value_counts().min()\n",
        "\n",
        "train_small = train_df.groupby(\"lang\").apply(\n",
        "    lambda x: x.sample(n=balanced_size_train, random_state=42)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "test_small = test_df.groupby(\"lang\").apply(\n",
        "    lambda x: x.sample(n=balanced_size_test, random_state=42)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# encode text data into bert tensors (x) and labels (y)\n",
        "train_enc, y_train = encode_batch(train_small)\n",
        "val_enc,   y_val   = encode_batch(val_small)\n",
        "test_enc,  y_test  = encode_batch(test_small)"
      ],
      "metadata": {
        "id": "2z6uSx0e5Qqk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset to hold bert inputs and labels\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# create dataset obj for train, val,test\n",
        "train_dataset = TextDataset(train_enc, y_train)\n",
        "val_dataset   = TextDataset(val_enc, y_val)\n",
        "test_dataset  = TextDataset(test_enc, y_test)\n",
        "\n",
        "# load data in small batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) #shuffle true to improve learning\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16)\n"
      ],
      "metadata": {
        "id": "LMzDN9OS5Qs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
      ],
      "metadata": {
        "id": "blTb8rTF5Qvc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"mbert_final.pt\")\n",
        "print(\"Training complete and model saved.\")\n"
      ],
      "metadata": {
        "id": "qvTuZomX5QyA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        all_preds += preds.cpu().tolist()\n",
        "        all_labels += batch[\"labels\"].cpu().tolist()\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "prec = precision_score(all_labels, all_preds)\n",
        "rec = recall_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "rdR3NEdU5Q0P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Human\", \"AI\"], yticklabels=[\"Human\", \"AI\"])\n",
        "plt.title(\"mBERT Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gtdb-E6Q5Q2u",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_per_language(model, df, tokenizer, device, batch_size=16):\n",
        "    label_map = {0: \"Human\", 1: \"AI\"}\n",
        "\n",
        "    for lang in [\"en\", \"zh\"]:\n",
        "        subset = df[df[\"lang\"] == lang].reset_index(drop=True)\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        model.eval()\n",
        "        # process the text in small batches\n",
        "        for i in range(0, len(subset), batch_size):\n",
        "            batch_texts = subset[\"text\"].iloc[i:i+batch_size].tolist()\n",
        "            batch_labels = subset[\"label\"].iloc[i:i+batch_size].tolist()\n",
        "            # tokenize the batch\n",
        "            encodings = tokenizer(\n",
        "                batch_texts,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=128,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**encodings)\n",
        "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(batch_labels)\n",
        "\n",
        "            torch.cuda.empty_cache()  # free GPU memory after each batch\n",
        "\n",
        "        # Compute confusion matrix and metrics\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        plt.figure(figsize=(4, 3))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                    xticklabels=[\"Human\", \"AI\"],\n",
        "                    yticklabels=[\"Human\", \"AI\"])\n",
        "        plt.title(f\"Confusion Matrix – {lang.upper()}\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.show()\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        prec = precision_score(all_labels, all_preds)\n",
        "        rec = recall_score(all_labels, all_preds)\n",
        "        f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "        print(f\"\\n===== {lang.upper()} Metrics =====\")\n",
        "        print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "plot_confusion_per_language(model, test_df, tokenizer, device, batch_size=16)\n",
        "\n"
      ],
      "metadata": {
        "id": "-MX0Y38b5Q5S",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mbert_samples(model, df, tokenizer, n=3):\n",
        "    \"\"\"Show sample predictions for both English and Chinese examples.\"\"\"\n",
        "    model.eval()\n",
        "    label_map = {0: \"Human\", 1: \"AI\"}\n",
        "\n",
        "    for lang in [\"en\", \"zh\"]:\n",
        "        subset = df[df[\"lang\"] == lang] # filter the test set for the target lang\n",
        "        sample_rows = subset.sample(n=min(n, len(subset)), random_state=42) # randomly select n number of examples to display\n",
        "        # tokenizer the selected text so it can be passed to mBert\n",
        "        encodings = tokenizer(\n",
        "            sample_rows[\"text\"].tolist(),\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encodings)\n",
        "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        # convert to readable label\n",
        "        sample_rows[\"Predicted\"] = [label_map[p] for p in preds]\n",
        "        sample_rows[\"Actual\"] = sample_rows[\"label\"].map(label_map)\n",
        "\n",
        "        print(f\"\\nSample predictions for {lang.upper()} ({len(sample_rows)} examples):\\n\")\n",
        "        for _, r in sample_rows.iterrows():\n",
        "            print(f\"Text: {r['text'][:120]}...\")\n",
        "            print(f\"→ Actual: {r['Actual']} | Predicted: {r['Predicted']}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "show_mbert_samples(model, test_df, tokenizer, n=5)\n"
      ],
      "metadata": {
        "id": "YuaH3J6l5Q8E",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analysis**\n",
        "\n",
        "Model 3 clearly outperforms both the Naive Bayes and FastText models on almost every metric.\n",
        "\n",
        "Overall Performance\n",
        "--------\n",
        "Accuracy: 0.9140\n",
        "\n",
        "Precision: 0.8795\n",
        "\n",
        "Recall: 0.9734\n",
        "\n",
        "F1-score: 0.9241\n",
        "\n",
        "The very high recall shows that mBERT is extremely good at detecting AI-generated text, and the strong F1-score confirms that it is much more reliable than the previous models. Unlike TF-IDF or FastText, mBERT uses full sentence context, which helps it recognize deeper writing patterns.\n",
        "\n",
        "------ English (EN) -------\n",
        "\n",
        "Accuracy: 0.8620\n",
        "\n",
        "F1-score: 0.8967\n",
        "\n",
        "English performance improves a lot compared to Models 1 and 2, but it is still not perfect.\n",
        "The confusion matrix shows:\n",
        "\n",
        "The model correctly identifies most AI texts (37,217 cases)\n",
        "\n",
        "But still confuses some human text as AI (11,959 cases)\n",
        "\n",
        "This reflects the complexity and variation in English writing styles. Even with mBERT, English remains more challenging than Chinese.\n",
        "\n",
        "----- Chinese (ZH) -------\n",
        "\n",
        "Accuracy: 0.9660\n",
        "\n",
        "F1-score: 0.9634\n",
        "\n",
        "The mBERT model performs extremely well on Chinese.\n",
        "From the confusion matrix:\n",
        "\n",
        "Only 13 AI texts are misclassified as human\n",
        "\n",
        "Only 217 human texts are misclassified as AI\n",
        "\n",
        "This is a very strong result and shows that mBERT handles Chinese patterns extremely well. The combination of contextual embeddings + Jieba tokenization helps mBERT capture meaning more accurately."
      ],
      "metadata": {
        "id": "a2AJnpM93X1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion for the project\n",
        "\n",
        "The three models show a clear progression in performance as the complexity of the approach increases. TF-IDF with Multinomial Naive Bayes provides a simple baseline but struggles to capture deeper linguistic patterns, particularly in English. FastText combined with an FNN improves performance by leveraging pretrained multilingual word embeddings, though its reliance on averaged vectors limits its ability to model full sentence context. Multilingual BERT achieves the strongest results, benefiting from contextualized representations and multilingual pretraining. Overall, mBERT demonstrates superior accuracy and generalization across both English and Chinese, making it the most effective model for multilingual AI-generated text detection in this study."
      ],
      "metadata": {
        "id": "M7oHQ4Ep5sEW"
      }
    }
  ]
}