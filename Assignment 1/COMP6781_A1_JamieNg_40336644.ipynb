{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamienkp/COMP6781/blob/main/Assignment%201/COMP6781_A1_JamieNg_40336644.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7rqxJ_b9-uD"
      },
      "source": [
        "## Assigment 1: Word2vec and CBOW âš¡\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWpKZPpdZcV8"
      },
      "source": [
        "In this assignment, we will guide you through the process of implementing Continous Bag Of Words (CBOW) using PyTorch, providing step-by-step instructions, code snippets, and explanations to aid your understanding. By the end of this assignment, you will have a solid grasp of Word2Vec's mechanics and be equipped to apply this knowledge to a wide range of NLP tasks.\n",
        "\n",
        "Let's embark on this journey to unlock the power of word embeddings with Word2Vec and PyTorch!\n",
        "\n",
        "This [guide](https://pytorch.org/tutorials/beginner/basics/intro.html) might come in handy to understand how Pytorch works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-kso1_rb2SM"
      },
      "source": [
        "Let's start by installing the needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aCFYPXdv_qZr"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKKX1cpcdVj-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaXq72-9S-__"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AADnCtQddOYv"
      },
      "source": [
        "In this assigment we will use the dataset *text8*. This dataset is composed of textual content extracted from Wikipedia articles which hopefully will lead to good embeddings when input in CBOW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZyk8pYS9xOJ"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "dataset = list(api.load(\"text8\"))[0:200] #We trim the dataset to make training as long. Feel free to change this based on your resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eia3XW39jWP"
      },
      "source": [
        "Let's have a look at a datapoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pkSsdyq9YHT"
      },
      "outputs": [],
      "source": [
        "(\" \").join(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTEtLyORIPZi"
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6I-utm7mh_N"
      },
      "source": [
        "Normally we would need to do some data preprocessing in order to clean our dataset but lucky for you the professor chose a pretty and clean dataset so you don't have to ðŸ˜‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUxUbk8nyTmu"
      },
      "source": [
        "Let's start to get more on topic. If you remember, Word2Vec has a specific way of generating each vector.\n",
        "![Captura de pantalla 2024-05-21 161613.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "The embedding is based on the surrounding words (before and after) taken as context. In the case of CBOW, we input the surrounding words to our neural network and expect that it predicts the middle word. Let's format our dataset to meet the needs of the neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oysoGd4Wxq8I"
      },
      "source": [
        "Since our neural networks can't understand strings we have to turn these strings into integers. In this case, we can use the one hot encodings to represent the words, so we will need the mapping: word -> one-hot-encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20IvUrxpwDL-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def create_word_to_index(vocab_list):\n",
        "  \"\"\" Method that creates dictionary that maps words to an index.\n",
        "  Arguments\n",
        "  ---------\n",
        "  vocab_list : list(String)\n",
        "    List of words in the vocabulary\n",
        "  Returns\n",
        "  -------\n",
        "  word_to_index : Dictionary\n",
        "    Dictionary mapping words to index with format {word:index}\n",
        "  \"\"\"\n",
        "  # TODO Write code that associates every word to a specific index. IMPORTANT: Remember to add the token OOV for words out of vocabulary\n",
        "\n",
        "  word2index = {'OOV': 0}\n",
        "\n",
        "  for i, word in enumerate(vocab_list, start=1):\n",
        "    word2index[word] = i\n",
        "\n",
        "\n",
        "  return word2index\n",
        "\n",
        "all_words = [word for sentence in dataset for word in sentence]\n",
        "\n",
        "vocab_list = list(set(all_words))\n",
        "word_to_index = create_word_to_index(vocab_list)\n",
        "\n",
        "\n",
        "print(\"Vocab list: \", vocab_list)\n",
        "print(\"Word to index: \", word_to_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OkVuGeFQ9Bk"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(dataset, window_size,word_to_index):\n",
        "\n",
        "\n",
        "  \"\"\" Method to generate training dataset for CBOW.\n",
        "  Arguments\n",
        "  ---------\n",
        "  data : String\n",
        "     Training dataset\n",
        "  window_size : int\n",
        "     Size of the context window\n",
        "  word_to_index : Dictionary\n",
        "     Dictionary mapping words to index with format {word:index}\n",
        "  Returns\n",
        "  -------\n",
        "  surroundings : N x W Tensor\n",
        "      Tensor with index of surrounding words, with N being the number of samples and W being the window size\n",
        "  targets : Tensor\n",
        "      Tensor with index of target word\n",
        "  \"\"\"\n",
        "  surroundings = []\n",
        "  targets = []\n",
        "\n",
        "  # TODO complete the following\n",
        "  for i in dataset:\n",
        "\n",
        "      words = i\n",
        "\n",
        "      if len(words) < 2 * window_size + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(window_size,len(words)-window_size):\n",
        "\n",
        "        context_words = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
        "        #get indexes of surrounding words in a list\n",
        "        surrounding = [word_to_index.get(w, word_to_index[\"OOV\"]) for w in context_words]\n",
        "        surrounding = torch.tensor(surrounding)\n",
        "        #get index of target word\n",
        "        target =  word_to_index.get(words[i], word_to_index[\"OOV\"])\n",
        "        surroundings.append((surrounding))\n",
        "        targets.append(target)\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  surroundings = torch.stack(surroundings)\n",
        "  targets = torch.tensor(targets)\n",
        "\n",
        "  return surroundings, targets\n",
        "\n",
        "\n",
        "WINDOW_SIZE = 2\n",
        "surroundings, targets = generate_dataset(dataset, WINDOW_SIZE, word_to_index)\n",
        "\n",
        "print(surroundings.shape)\n",
        "print(targets.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OYFIK0g0YsI"
      },
      "source": [
        "With our dataset ready we can finally create our Neural Network. The idea is to replicate what Mikolov did in 2013 (see slides of Word2Vec)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKDcC75jzaXz"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim=300):\n",
        "    \"\"\" Class to define the CBOW model\n",
        "    Attributes\n",
        "    ---------\n",
        "    device : device\n",
        "      Device where the model will be trained (gpu preferably)\n",
        "    vocab_size : int\n",
        "      Size of the vocabulary\n",
        "    embed_dim : int\n",
        "      Size of the embedding layer\n",
        "    hidden_dim : int\n",
        "      Size of the hidden layer\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    # Linear layer that encodes one-hot-encoding to embedding dimension. Remember Mikolov's implementation had NO BIAS\n",
        "    self.linear_in = nn.Linear(vocab_size, embed_dim, bias=False)\n",
        "    #  Linear layer that decodes from embedding dimension to vocab dimension. Remember Mikolov's implementation had NO BIAS\n",
        "    self.linear_out = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #pass input through embedding layer\n",
        "    emb = self.linear_in(x)\n",
        "    #average embeddings\n",
        "    average = emb.mean(dim=1)\n",
        "    #pass through linear layer\n",
        "    out = self.linear_out(average)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp7Z3KGw_vIQ"
      },
      "source": [
        "We will need an important function that creates one hot encoding vectors based on our surrounding word's indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqaffIc_9l5o"
      },
      "outputs": [],
      "source": [
        "def one_hot_from_surroundings(surroundings,window_size, vocab_size):\n",
        "  \"\"\" Method to create one hot encodings for the surrounding words.\n",
        "  Arguments\n",
        "  ---------\n",
        "  surroundings : N x (Wx2) Tensor\n",
        "     Tensor with index of surrounding words, with N being the number of samples and W being the window size\n",
        "  vocab_size : int\n",
        "     Size of the vocabulary\n",
        "  Returns\n",
        "  -------\n",
        "  one_hot_surroundings : N x (Wx2) x V Tensor\n",
        "      Tensor with one hot encodings of surrounding words, with N being the number of samples, W being the window size and V being the vocabulary size\n",
        "  \"\"\"\n",
        "  # TODO write function\n",
        "\n",
        "  # surrounding is a samples of words that is around the target\n",
        "  one_hot_surr = []\n",
        "  for surr in surroundings:\n",
        "    # for each surround\n",
        "    surr_one_hot = []\n",
        "    for word in surr:\n",
        "      # for each word in each surround\n",
        "      one_hot = torch.zeros(vocab_size, device=device)    # initialise one hot as 0 with the size of vocab size\n",
        "      one_hot[word] = 1\n",
        "      surr_one_hot.append(one_hot)\n",
        "    one_hot_surr.append(torch.stack(surr_one_hot))\n",
        "\n",
        "  return torch.stack(one_hot_surr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON_SHTKf-bY-"
      },
      "outputs": [],
      "source": [
        "#Check if your previous function is correct\n",
        "index= [[2,3,7,8],[0,1,9,5]]\n",
        "index_tensor = torch.tensor(index).to(device)\n",
        "answer = torch.tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
        "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
        "\n",
        "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
        "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]], device=device)\n",
        "\n",
        "prediction = one_hot_from_surroundings(index_tensor,2,10)\n",
        "assert prediction.shape == torch.Size([2, 4, 10]), \"Make sure the result has shape NUM_EXAMPLES X WINDOW_SIZE*2 X VOCAB_SIZE\"\n",
        "assert torch.equal(prediction,answer), \"Wrong implementation of one_hot_from_surroundings method\"\n",
        "print(\"Good Job!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9swEtZqAtVw"
      },
      "source": [
        "Now we are finally ready to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F9PVmiIgsXW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset, random_split\n",
        "#creation of dataloader for training\n",
        "full_dataset = list(zip(surroundings,targets))\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=64,shuffle=True) #Here please change batch size depending of your GPU capacities (if GPU runs out of memory lower batch_size)\n",
        "val_dataloader=DataLoader(val_dataset,batch_size=64,shuffle=False) #Here please change batch size depending of your GPU capacities (if GPU runs out of memory lower batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyMPIPX-51l8"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #CAUTION: RUN THIS CODE WITH GPU, CPU WILL TAKE TOO LONG\n",
        "EMB_DIM = 300\n",
        "CBOW_MODEL = CBOW(len(word_to_index), embed_dim= EMB_DIM).to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(CBOW_MODEL.parameters(), lr=0.001)\n",
        "EPOCHS = 1 #Question1\n",
        "\n",
        "#BE PATIENT: This code can take up to 1 hours for a batch size of 512 and 1 epochs\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=1, lr=1e-3): #Question1\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for surr, tar in tqdm(train_dataloader, \"Training\"):\n",
        "        surr = surr.to(device)\n",
        "        tar  = tar.to(device)\n",
        "\n",
        "        # N x (2*W) x V one-hot tensor\n",
        "        one_hot_surr = one_hot_from_surroundings(\n",
        "            surr, WINDOW_SIZE, vocab_size=len(word_to_index)\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #  N x V\n",
        "        logits = model(one_hot_surr)\n",
        "        loss = loss_function(logits, tar)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for surr, tar in tqdm(val_loader, \"Validation\"):\n",
        "            surr = surr.to(device)\n",
        "            tar  = tar.to(device)\n",
        "\n",
        "            one_hot_surr = one_hot_from_surroundings(\n",
        "                surr, WINDOW_SIZE, vocab_size=len(word_to_index)\n",
        "            ).to(device)\n",
        "\n",
        "            logits = model(one_hot_surr)\n",
        "            loss = loss_function(logits, tar)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = train_model(CBOW_MODEL, train_dataloader, val_dataloader, epochs=1, lr=0.001)\n",
        "  val_loss = evaluate_model(CBOW_MODEL, val_dataloader)\n",
        "  print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTXzmBZNdMrL"
      },
      "outputs": [],
      "source": [
        "# save model with epoch 5\n",
        "# torch.save(CBOW_MODEL.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/NLP-Assignment1/cbow_model_weights_with_epoch5.pth\")\n",
        "\n",
        "# save model with epochs 6â€º\n",
        "# torch.save(CBOW_MODEL.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/NLP-Assignment1/cbow_model_weights_with_epoch6.pth\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyLJA98sdOur"
      },
      "outputs": [],
      "source": [
        "# RUN ONLY ONCE\n",
        "\n",
        "# torch.save({\n",
        "#     'epoch': epoch,\n",
        "#     'model_state_dict': CBOW_MODEL.state_dict(),\n",
        "#     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#     'loss': train_loss,\n",
        "#     'word_to_index': word_to_index,\n",
        "# }, \"/content/drive/MyDrive/Colab Notebooks/NLP-Assignment1/cbow_checkpoint_epoch5.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw1EXciKVt8T"
      },
      "outputs": [],
      "source": [
        " checkpoint = torch.load(\"/content/drive/MyDrive/Colab Notebooks/NLP-Assignment1/cbow_checkpoint_epoch5.pth\", map_location=device)\n",
        "\n",
        " CBOW_MODEL = CBOW(len(checkpoint['word_to_index']), embed_dim=300).to(device)\n",
        " CBOW_MODEL.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        " optimizer = torch.optim.Adam(CBOW_MODEL.parameters(), lr=0.001)\n",
        " optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        " word_to_index = checkpoint['word_to_index']\n",
        " epoch = checkpoint['epoch']\n",
        " train_loss = checkpoint['loss']\n",
        "\n",
        " print(f\"Loaded model trained up to epoch {epoch} with loss {train_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMjs6zu63bnh"
      },
      "source": [
        "## Let's test it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rarP3Yw3eCW"
      },
      "source": [
        "Now that we hopefully have good embeddings we can put them to the test. Let's start by creating some useful functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xJndx_85UJeY"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(word_to_index)\n",
        "\n",
        "def create_word_to_embedding(vocab_list, model, word_to_index):\n",
        "    \"\"\" Method to get create dictionary that maps a word to it's embedding vector.\n",
        "    Arguments\n",
        "    ---------\n",
        "    word : String\n",
        "      Word given\n",
        "    model : NN.module\n",
        "      CBOW model\n",
        "    word_to_index : Dictionary\n",
        "      Dictionary mapping words to index with format {word:index}\n",
        "    Returns\n",
        "    -------\n",
        "    word_embedding : Tensor\n",
        "        Embedding vector for the given word\n",
        "    \"\"\"\n",
        "\n",
        "    W = model.linear_in.weight.detach().cpu().t()   # Get the weights of the embedding layer\n",
        "\n",
        "    word_to_embedding = {}\n",
        "    for word in vocab_list:\n",
        "        index = word_to_index.get(word, word_to_index[\"OOV\"])\n",
        "\n",
        "        if 0 <= index < W.size(0):\n",
        "            word_to_embedding[word] = W[index]\n",
        "        else:\n",
        "            word_to_embedding[word] = W[word_to_index[\"OOV\"]]\n",
        "    return word_to_embedding\n",
        "\n",
        "word_to_embedding = create_word_to_embedding(vocab_list, CBOW_MODEL, word_to_index)\n",
        "print(\"Embedding of word 'study':\", word_to_embedding.get('Croatia')).  # Question 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d_EwIyfptj5"
      },
      "source": [
        "We will test how good our word2vec model is by testing the embeddings on the downstream task of sentiment analysis of movies\n",
        "\n",
        "Let's first import and download this dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPK_JPPYVY4M"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "def decode_review(word_index, review):\n",
        "  decoded_review = \" \".join([reverse_word_index.get(i - 3, \"\") for i in review])\n",
        "  return decoded_review\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels)= imdb.load_data(num_words=10000)\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "# We will trim the dataset to not make training to long, feel free to change this based on your resources\n",
        "train_data = train_data[:10000]\n",
        "train_labels = train_labels[:10000]\n",
        "test_data = test_data[:2000]\n",
        "test_labels = test_labels[:2000]\n",
        "\n",
        "#Decode data\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "train_data = [decode_review(reverse_word_index,x) for x in train_data]\n",
        "test_data = [decode_review(reverse_word_index,x) for x in test_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubc_mCGE62JB"
      },
      "source": [
        "Now we can create a simple Sentiment classifier that will use our embeddings from our CBOW model. The model input will be an average of the word embeddings of the review and the output will be a binary: positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "207hqm-ZC95U"
      },
      "source": [
        "Let's start by creating our Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UPj6em3dDCXU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_review_embedding(review, word_to_embedding):\n",
        "    \"\"\" Method to get the embedding vector for a given review.\n",
        "    Arguments\n",
        "    ---------\n",
        "    review : String\n",
        "      Review given\n",
        "    word_to_embedding : Dictionary\n",
        "      Dictionary mapping words to it's embedding with format {word:embedding}\n",
        "    Returns\n",
        "    -------\n",
        "    review_embedding : Tensor\n",
        "        Embedding vector for the given review\n",
        "    \"\"\"\n",
        "    # Split the review into words\n",
        "    words = review.lower().split()\n",
        "\n",
        "    # Collect embeddings for words that exist in our vocabulary\n",
        "    valid_embeddings = [\n",
        "        word_to_embedding[w]\n",
        "        for w in words\n",
        "        if w in word_to_embedding\n",
        "    ]\n",
        "\n",
        "    # If no words from the review are in the vocab, return a zero vector\n",
        "    if len(valid_embeddings) == 0:\n",
        "        # get dimension from any embedding\n",
        "        emb_dim = next(iter(word_to_embedding.values())).shape[0]\n",
        "        return torch.zeros(emb_dim)\n",
        "\n",
        "    # Stack and average\n",
        "    review_embedding = torch.stack(valid_embeddings).mean(dim=0)\n",
        "\n",
        "    return review_embedding\n",
        "\n",
        "\n",
        "def create_dataloader_imdb(train_data, train_labels, test_data, test_labels, word_to_embedding, batch_size=64):\n",
        "    \"\"\" Method to create dataloaders for the IMDB dataset\n",
        "    Arguments\n",
        "    ---------\n",
        "    train_data : List\n",
        "      List of reviews for training\n",
        "      train_labels : List\n",
        "      List of labels for training\n",
        "    test_data : List\n",
        "      List of reviews for testing\n",
        "    test_labels : List\n",
        "      List of labels for testing\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    \"\"\"\n",
        "    train_dataset = []\n",
        "    test_dataset = []\n",
        "\n",
        "    for i, review in enumerate(train_data):\n",
        "        review_emb = get_review_embedding(review, word_to_embedding)\n",
        "        train_dataset.append((review_emb, torch.tensor(train_labels[i], dtype=torch.long)))\n",
        "\n",
        "    for i, review in enumerate(test_data):\n",
        "        review_emb = get_review_embedding(review, word_to_embedding)\n",
        "        test_dataset.append((review_emb, torch.tensor(test_labels[i], dtype=torch.long)))\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "\n",
        "train_dataloader_imd, test_dataloader_imd = create_dataloader_imdb(train_data, train_labels, test_data, test_labels, word_to_embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYly1Y6YCGJl"
      },
      "source": [
        "Let's create a very simple MLP for this classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrzjadMnGa8C"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim=128, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49_1rzXRCViG"
      },
      "source": [
        "Now the fun part, time to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lm-IYgjYQdX"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  #CAUTION: RUN THIS CODE WITH GPU, CPU WILL TAKE TOO LONG\n",
        "SENT_MODEL = SentimentClassifier(embedding_dim=EMB_DIM).to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(SENT_MODEL.parameters(), lr=0.001)\n",
        "EPOCHS = 5\n",
        "\n",
        "def train_model(model, train_loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for X, y in tqdm(train_loader, \"Training\"):\n",
        "        # Move data to GPU if available\n",
        "        X = X.to(device).float()\n",
        "        y = y.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)                  # forward\n",
        "        loss = loss_function(logits, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm(val_loader, \"Validation\"):\n",
        "            X = X.to(device).float()\n",
        "            y = y.to(device).long()\n",
        "\n",
        "            logits = model(X)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            preds = probs.argmax(dim=1)\n",
        "\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "            all_predictions.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(y.cpu().tolist())\n",
        "\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    return acc, all_predictions, all_labels\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_model(SENT_MODEL, train_dataloader_imd)\n",
        "    validation_accuracy, predictions, labels = evaluate_model(SENT_MODEL, test_dataloader_imd)\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4bKgugqAtuW"
      },
      "source": [
        "Calculate precision, recall and F-measure of our Sentiment Analysis model. Also plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbCx4unQfSHP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Convert to numpy arrays if not already\n",
        "y_true = np.array(labels)\n",
        "y_pred = np.array(predictions)\n",
        "\n",
        "# Calculate metrics\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# Compute and display confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title(\"Confusion Matrix - Sentiment Classifier\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8069Ltpe6sQc"
      },
      "source": [
        "# Let's Experiment! ðŸ¤“\n",
        "\n",
        "Pick 3 of the following experiments and develop a solution **in this same notebook**. Take advange of the notebook structure to give a clear structure of experiment -> conclusions\n",
        "\n",
        "1. Compare with another pretrained word2vec model\n",
        "2. Experiment with a different sentiment analysis dataset (might require change in architecture of sentiment analyser)\n",
        "3. Train your CBOW model on a different train set (might require data preprocessing)\n",
        "4. Play with hyperparameters (window size, embedding dimension)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RtspVFxcHjb"
      },
      "source": [
        "# Experiment 1\n",
        " Use another pretrained word2vec model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "oX8zQljjzEfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziO1_O6tcOs1"
      },
      "outputs": [],
      "source": [
        "import re, numpy as np, torch, torch.nn as nn, torch.nn.functional as F, matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import gensim.downloader as api\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CBOW_CKPT_PATH = \"/content/drive/MyDrive/Colab Notebooks/NLP-Assignment1/cbow_checkpoint_epoch5.pth\"\n",
        "\n",
        "# Load trained CBOW model\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, V, D=300):\n",
        "        super().__init__()\n",
        "        self.linear_in = nn.Linear(V, D, bias=False)\n",
        "        self.linear_out = nn.Linear(D, V, bias=False)\n",
        "    def forward(self, x): return self.linear_out(self.linear_in(x).mean(1))\n",
        "\n",
        "ckpt = torch.load(CBOW_CKPT_PATH, map_location=device)\n",
        "word_to_index = ckpt[\"word_to_index\"]\n",
        "CBOW_MODEL = CBOW(len(word_to_index), 300).to(device)\n",
        "CBOW_MODEL.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "CBOW_MODEL.eval()\n",
        "\n",
        "# Extract CBOW embeddings\n",
        "@torch.no_grad()\n",
        "def cbow_map():\n",
        "    W = CBOW_MODEL.linear_in.weight.detach().cpu().t()\n",
        "    d = {w: W[i] for w, i in word_to_index.items()}\n",
        "    d.setdefault(\"OOV\", W[word_to_index.get(\"OOV\", 0)])\n",
        "    return d\n",
        "word_to_emb_cbow = cbow_map()\n",
        "\n",
        "# Load pretrained embeddings using glove\n",
        "pre_kv = api.load(\"glove-wiki-gigaword-100\")\n",
        "word_to_emb_pre = {\"OOV\": torch.zeros(pre_kv.vector_size)} | {\n",
        "    w: torch.tensor(pre_kv[w]) for w in pre_kv.key_to_index\n",
        "}\n",
        "print(\"CBOW and Pretrained GloVe loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0z_ZZI3i9X7"
      },
      "outputs": [],
      "source": [
        "# Dataset and Feature Preparation\n",
        "(train_X, train_y), (test_X, test_y) = imdb.load_data(num_words=10000)\n",
        "rev = {v: k for k, v in imdb.get_word_index().items()}\n",
        "decode = lambda ids: \" \".join(rev.get(i - 3, \"\") for i in ids).lower()\n",
        "\n",
        "# Use smaller subset for faster training\n",
        "train_texts = [decode(x) for x in train_X[:10000]]\n",
        "test_texts  = [decode(x) for x in test_X[:2000]]\n",
        "train_y, test_y = np.array(train_y[:10000]), np.array(test_y[:2000])\n",
        "\n",
        "# Convert review text â†’ averaged embedding vector\n",
        "def review_vec(text, emb_map):\n",
        "    toks = re.findall(r\"[a-z']+\", text)\n",
        "    vecs = [emb_map[w] for w in toks if w in emb_map]\n",
        "    return torch.stack(vecs).mean(0) if vecs else torch.zeros(next(iter(emb_map.values())).shape[0])\n",
        "\n",
        "def make_loader(txts, ys, emb_map, bs=64):\n",
        "    data = [(review_vec(t, emb_map), torch.tensor(y).long()) for t, y in zip(txts, ys)]\n",
        "    return DataLoader(data, batch_size=bs, shuffle=True)\n",
        "\n",
        "train_cbow = make_loader(train_texts, train_y, word_to_emb_cbow)\n",
        "test_cbow  = make_loader(test_texts, test_y, word_to_emb_cbow)\n",
        "train_pre  = make_loader(train_texts, train_y, word_to_emb_pre)\n",
        "test_pre   = make_loader(test_texts, test_y, word_to_emb_pre)\n",
        "\n",
        "print(\"IMDB dataset prepared and converted into embeddings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Classifiers\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, d, h=128):\n",
        "        super().__init__()\n",
        "        self.fc1, self.fc2, self.drop = nn.Linear(d, h), nn.Linear(h, 2), nn.Dropout(0.3)\n",
        "    def forward(self, x): return self.fc2(self.drop(F.relu(self.fc1(x))))\n",
        "\n",
        "def train_and_eval(dim, train_loader, test_loader, epochs=3):\n",
        "    m = SentimentClassifier(dim).to(device)\n",
        "    opt = torch.optim.Adam(m.parameters(), 1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for _ in range(epochs):\n",
        "        m.train()\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device).float(), y.to(device)\n",
        "            opt.zero_grad(); loss = loss_fn(m(X), y); loss.backward(); opt.step()\n",
        "    # evaluate\n",
        "    m.eval(); preds, gold = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X = X.to(device).float()\n",
        "            p = m(X).argmax(1).cpu().numpy()\n",
        "            preds += list(p); gold += list(y.numpy())\n",
        "    p = precision_score(gold, preds, average='binary')\n",
        "    r = recall_score(gold, preds, average='binary')\n",
        "    f = f1_score(gold, preds, average='binary')\n",
        "    return p, r, f\n",
        "\n",
        "#Train both models\n",
        "p_cbow, r_cbow, f_cbow = train_and_eval(300, train_cbow, test_cbow, 3)\n",
        "p_pre,  r_pre,  f_pre  = train_and_eval(pre_kv.vector_size, train_pre, test_pre, 3)\n",
        "\n",
        "print(f\"CBOW   â†’ Precision: {p_cbow:.4f} | Recall: {r_cbow:.4f} | F1: {f_cbow:.4f}\")\n",
        "print(f\"PreTr. â†’ Precision: {p_pre:.4f}  | Recall: {r_pre:.4f}  | F1: {f_pre:.4f}\")\n",
        "\n",
        "# Bar chart comparison\n",
        "metrics = [\"Precision\", \"Recall\", \"F1\"]\n",
        "cbow_vals, pre_vals = [p_cbow, r_cbow, f_cbow], [p_pre, r_pre, f_pre]\n",
        "x = np.arange(3); w = 0.35\n",
        "plt.figure(figsize=(6,3.4))\n",
        "plt.bar(x - w/2, cbow_vals, w, label=\"CBOW\", color=\"blue\")\n",
        "plt.bar(x + w/2, pre_vals,  w, label=\"Pretrained\", color=\"orange\")\n",
        "plt.xticks(x, metrics); plt.ylim(0,1)\n",
        "plt.ylabel(\"Score\"); plt.title(\"IMDB Sentiment â€” CBOW vs Pretrained\")\n",
        "plt.legend(); plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WwXHQ654yadn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Nearest-Neighbor Visualization using cosine similarity\n",
        "@torch.no_grad()\n",
        "def cbow_neighbors(w, k=8):\n",
        "    i = word_to_index.get(w) or word_to_index.get(w.lower())\n",
        "    if i is None: return []\n",
        "    W = CBOW_MODEL.linear_in.weight.detach().cpu().t()\n",
        "    sims = F.cosine_similarity(W[i].unsqueeze(0), W).squeeze(0); sims[i] = -1\n",
        "    idx = sims.topk(min(k, W.size(0)-1)).indices.tolist()\n",
        "    inv = {j: u for u, j in word_to_index.items()}\n",
        "    return [(inv.get(j, f\"IDX_{j}\"), float(sims[j])) for j in idx]\n",
        "\n",
        "def pre_neighbors(w, k=8):\n",
        "    w = w if w in pre_kv else w.lower()\n",
        "    try: return pre_kv.most_similar(w, topn=k)\n",
        "    except KeyError: return []\n",
        "\n",
        "def plot_neighbors(word, k=8):\n",
        "    cb, pr = cbow_neighbors(word, k), pre_neighbors(word, k)\n",
        "    if not cb and not pr: return print(\"Word not found.\")\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(9.5, 4))\n",
        "    plt.suptitle(f\"Most Similar to '{word}'\", weight='bold')\n",
        "    for a, (ws, s, title, col) in enumerate((( [w for w,_ in cb], [v for _,v in cb], \"CBOW\", \"blue\"),\n",
        "                                             ( [w for w,_ in pr], [v for _,v in pr], \"Pretrained\", \"orange\"))):\n",
        "        if ws: ax[a].barh(range(len(ws))[::-1], s[::-1], color=col)\n",
        "        ax[a].set_yticks(range(len(ws))[::-1]); ax[a].set_yticklabels(ws[::-1])\n",
        "        ax[a].set_xlim(0,1); ax[a].set_title(title); ax[a].set_xlabel(\"Cosine\"); ax[a].grid(axis='x', alpha=0.2)\n",
        "    plt.tight_layout(rect=[0,0,1,0.92]); plt.show()\n",
        "\n",
        "\n",
        "plot_neighbors(\"Latvia\", 8). # Question 3\n",
        "plot_neighbors(\"Belarus\", 8)\n",
        "plot_neighbors(\"good\", 8)\n"
      ],
      "metadata": {
        "id": "6cKNXUDmytVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdyG1oAccRFu"
      },
      "source": [
        "# Experiment 2\n",
        " Experiment with a different sentiment analysis dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-w2qa09ceEi"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74USJiGK4YsF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXhYXGh14aBc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gensim.downloader as api\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = list(api.load(\"text8\"))[0:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5HmrOhZ4av2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim=300):\n",
        "    \"\"\" Class to define the CBOW model\n",
        "    Attributes\n",
        "    ---------\n",
        "    device : device\n",
        "      Device where the model will be trained (gpu preferably)\n",
        "    vocab_size : int\n",
        "      Size of the vocabulary\n",
        "    embed_dim : int\n",
        "      Size of the embedding layer\n",
        "    hidden_dim : int\n",
        "      Size of the hidden layer\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "\n",
        "    # Linear layer that encodes one-hot-encoding to embedding dimension. Remember Mikolov's implementation had NO BIAS\n",
        "    self.linear_in = nn.Linear(vocab_size, embed_dim, bias=False)\n",
        "    #  Linear layer that decodes from embedding dimension to vocab dimension. Remember Mikolov's implementation had NO BIAS\n",
        "    self.linear_out = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #pass input through embedding layer\n",
        "    emb = self.linear_in(x)\n",
        "    #average embeddings\n",
        "    average = emb.mean(dim=1)\n",
        "    #pass through linear layer\n",
        "    out = self.linear_out(average)\n",
        "\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7KHCJPS4cD-"
      },
      "outputs": [],
      "source": [
        "# load saved model CBOW learning weight\n",
        "\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/Colab Notebooks/NLP-Assignment1/cbow_checkpoint_epoch5.pth\", map_location=device)\n",
        "\n",
        "CBOW_MODEL = CBOW(len(checkpoint['word_to_index']), embed_dim=300).to(device)\n",
        "CBOW_MODEL.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "optimizer = torch.optim.Adam(CBOW_MODEL.parameters(), lr=0.001)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "word_to_index = checkpoint['word_to_index']\n",
        "epoch = checkpoint['epoch']\n",
        "train_loss = checkpoint['loss']\n",
        "\n",
        "print(f\"Loaded model trained up to epoch {epoch} with loss {train_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW4Wd5cH4dEX"
      },
      "outputs": [],
      "source": [
        "# Reuse the same CBOW and vocab\n",
        "VOCAB_SIZE = len(word_to_index)\n",
        "\n",
        "def create_word_to_embedding(vocab_list, model, word_to_index):\n",
        "    W = model.linear_in.weight.detach().cpu().t()\n",
        "    word_to_embedding = {}\n",
        "    for word in vocab_list:\n",
        "        index = word_to_index.get(word, word_to_index[\"OOV\"])\n",
        "        if 0 <= index < W.size(0):\n",
        "            word_to_embedding[word] = W[index]\n",
        "        else:\n",
        "            word_to_embedding[word] = W[word_to_index[\"OOV\"]]\n",
        "    return word_to_embedding\n",
        "\n",
        "word_to_embedding = create_word_to_embedding(list(word_to_index.keys()), CBOW_MODEL, word_to_index)\n",
        "print(\"Embedding of word 'profit':\", word_to_embedding.get('profit', None))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dDp1Lnc4eRO"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "path = kagglehub.dataset_download(\"sbhatti/financial-sentiment-analysis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "df_finance = pd.read_csv(os.path.join(path, \"data.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(s):\n",
        "    s = str(s).lower()\n",
        "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# Column pickers (robust to variations)\n",
        "text_col_candidates  = [\"Sentence\", \"sentence\", \"text\"]\n",
        "label_col_candidates = [\"Sentiment\", \"sentiment\", \"label\"]\n",
        "\n",
        "def pick_col(df, candidates):\n",
        "    for c in candidates:\n",
        "        if c in df.columns: return c\n",
        "    raise KeyError(f\"Expected one of {candidates} in columns: {df.columns.tolist()}\")\n",
        "\n",
        "TEXT_COL  = pick_col(df_finance, text_col_candidates)\n",
        "LABEL_COL = pick_col(df_finance, label_col_candidates)\n",
        "\n",
        "# Normalize labels\n",
        "def norm_label(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in [\"neg\",\"negative\",\"-1\",\"bearish\"]: return 0\n",
        "    if s in [\"neu\",\"neutral\",\"0\"]:             return 1\n",
        "    if s in [\"pos\",\"positive\",\"1\",\"bullish\"]:  return 2\n",
        "    # FiQA numeric fallback\n",
        "    try:\n",
        "        v = float(s)\n",
        "        if v < -0.2: return 0\n",
        "        if v >  0.2: return 2\n",
        "        return 1\n",
        "    except:\n",
        "        return 1\n",
        "\n",
        "df_finance = df_finance[[TEXT_COL, LABEL_COL]].dropna().copy()\n",
        "df_finance[TEXT_COL] = df_finance[TEXT_COL].apply(clean_text)\n",
        "df_finance[\"y\"] = df_finance[LABEL_COL].apply(norm_label)\n",
        "\n",
        "print(df_finance[[TEXT_COL, \"y\"]].head())\n",
        "print(\"Label distribution:\\n\", df_finance[\"y\"].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "id": "lk_QWivXcVPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df_finance[\"Sentence\"].tolist(),\n",
        "    df_finance[\"y\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df_finance[\"y\"]\n",
        ")\n",
        "\n",
        "print(\"Train samples:\", len(train_texts))\n",
        "print(\"Test samples:\", len(test_texts))\n",
        "\n"
      ],
      "metadata": {
        "id": "js3EekMLcXEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_sentence_embedding(sentence, word_to_embedding):\n",
        "    words = sentence.split()\n",
        "    valid_embeddings = [word_to_embedding[w] for w in words if w in word_to_embedding]\n",
        "    if len(valid_embeddings) == 0:\n",
        "        emb_dim = next(iter(word_to_embedding.values())).shape[0]\n",
        "        return torch.zeros(emb_dim)\n",
        "    return torch.stack(valid_embeddings).mean(dim=0)\n",
        "\n",
        "def create_dataloader_finance(train_texts, train_labels, test_texts, test_labels, word_to_embedding, batch_size=64):\n",
        "    train_dataset, test_dataset = [], []\n",
        "    for i, s in enumerate(train_texts):\n",
        "        emb = get_sentence_embedding(s, word_to_embedding)\n",
        "        train_dataset.append((emb, torch.tensor(train_labels[i], dtype=torch.long)))\n",
        "    for i, s in enumerate(test_texts):\n",
        "        emb = get_sentence_embedding(s, word_to_embedding)\n",
        "        test_dataset.append((emb, torch.tensor(test_labels[i], dtype=torch.long)))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_dataloader_fin, test_dataloader_fin = create_dataloader_finance(\n",
        "    train_texts, train_labels, test_texts, test_labels, word_to_embedding\n",
        ")\n"
      ],
      "metadata": {
        "id": "yHBj8vgicYOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim=128, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Setup\n",
        "EMB_DIM = next(iter(word_to_embedding.values())).shape[0]\n",
        "SENT_MODEL = SentimentClassifier(embedding_dim=EMB_DIM, num_classes=3).to(device)"
      ],
      "metadata": {
        "id": "2VX6NzE0cbJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(SENT_MODEL.parameters(), lr=1e-3)\n",
        "EPOCHS = 5\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for X, y in tqdm(train_loader, \"Training\"):\n",
        "        X = X.to(device).float()\n",
        "        y = y.to(device).long()\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = loss_function(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / max(1, len(train_loader))\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    for X, y in tqdm(val_loader, \"Validation\"):\n",
        "        X = X.to(device).float()\n",
        "        y = y.to(device).long()\n",
        "        logits = model(X)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(y.cpu().tolist())\n",
        "    acc = correct / total if total else 0.0\n",
        "    return acc, all_preds, all_labels\n",
        "\n",
        "best_acc, best_state = 0.0, None\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    tr_loss = train_model(SENT_MODEL, train_dataloader_fin)\n",
        "    val_acc, preds, labs = evaluate_model(SENT_MODEL, test_dataloader_fin)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc, best_state = val_acc, {k: v.cpu() for k, v in SENT_MODEL.state_dict().items()}\n",
        "    print(f\"Epoch {ep} | Train Loss: {tr_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Load best checkpoint\n",
        "if best_state is not None:\n",
        "    SENT_MODEL.load_state_dict(best_state)\n"
      ],
      "metadata": {
        "id": "IhGK9JtucdsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "y_true = np.array(labs)        # true labels\n",
        "y_pred = np.array(preds)       # predicted labels\n",
        "\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall    = recall_score(y_true, y_pred, average='macro')\n",
        "f1        = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# ---- Confusion Matrix ----\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title(\"Confusion Matrix - Finance Sentiment Classifier\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y7FkOu6tcgH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq3NcQVKceLm"
      },
      "source": [
        "# Experiment 3\n",
        "\n",
        "Play with hyperparameters (window size, embedding dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCae1XjR5CHn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "GjOMv59QdPte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WnQOTZlqdRtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gensim.downloader as api\n",
        "dataset = list(api.load(\"text8\"))[0:50] #We trim the dataset to make training as long. Feel free to change this based on your resources"
      ],
      "metadata": {
        "id": "GzF2IMtZdST4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_word_to_index(vocab_list):\n",
        "  \"\"\" Method that creates dictionary that maps words to an index.\n",
        "  Arguments\n",
        "  ---------\n",
        "  vocab_list : list(String)\n",
        "    List of words in the vocabulary\n",
        "  Returns\n",
        "  -------\n",
        "  word_to_index : Dictionary\n",
        "    Dictionary mapping words to index with format {word:index}\n",
        "  \"\"\"\n",
        "  # TODO Write code that associates every word to a specific index. IMPORTANT: Remember to add the token OOV for words out of vocabulary\n",
        "\n",
        "  word2index = {'OOV': 0}\n",
        "\n",
        "  for i, word in enumerate(vocab_list, start=1):\n",
        "    word2index[word] = i\n",
        "\n",
        "  return word2index\n",
        "\n",
        "\n",
        "all_words = [word for sentence in dataset for word in sentence]\n",
        "\n",
        "vocab_list = list(set(all_words))\n",
        "word_to_index = create_word_to_index(vocab_list)\n",
        "\n",
        "print(\"Vocab list: \", vocab_list)\n",
        "print(\"Word to index: \", word_to_index)\n",
        "\n",
        "\n",
        "def generate_dataset(dataset, window_size, word_to_index):\n",
        "  \"\"\" Method to generate training dataset for CBOW.\n",
        "  Arguments\n",
        "  ---------\n",
        "  data : String\n",
        "     Training dataset\n",
        "  window_size : int\n",
        "     Size of the context window\n",
        "  word_to_index : Dictionary\n",
        "     Dictionary mapping words to index with format {word:index}\n",
        "  Returns\n",
        "  -------\n",
        "  surroundings : N x W Tensor\n",
        "      Tensor with index of surrounding words, with N being the number of samples and W being the window size\n",
        "  targets : Tensor\n",
        "      Tensor with index of target word\n",
        "  \"\"\"\n",
        "  surroundings = []\n",
        "  targets = []\n",
        "\n",
        "  # TODO complete the following\n",
        "  for i in dataset:\n",
        "\n",
        "      words = i\n",
        "\n",
        "      if len(words) < 2 * window_size + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(window_size, len(words) - window_size):\n",
        "\n",
        "        context_words = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
        "        #get indexes of surrounding words in a list\n",
        "        surrounding = [word_to_index.get(w, word_to_index[\"OOV\"]) for w in context_words]\n",
        "        surrounding = torch.tensor(surrounding)\n",
        "        #get index of target word\n",
        "        target =  word_to_index.get(words[i], word_to_index[\"OOV\"])\n",
        "        surroundings.append((surrounding))\n",
        "        targets.append(target)\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  surroundings = torch.stack(surroundings)\n",
        "  targets = torch.tensor(targets)\n",
        "\n",
        "  return surroundings, targets\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim=300):\n",
        "    \"\"\" Class to define the CBOW model\n",
        "    Attributes\n",
        "    ---------\n",
        "    device : device\n",
        "      Device where the model will be trained (gpu preferably)\n",
        "    vocab_size : int\n",
        "      Size of the vocabulary\n",
        "    embed_dim : int\n",
        "      Size of the embedding layer\n",
        "    hidden_dim : int\n",
        "      Size of the hidden layer\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    # Embedding lookup replaces one-hot + Linear\n",
        "    self.emb = nn.Embedding(vocab_size, embed_dim)\n",
        "    #  Linear layer that decodes from embedding dimension to vocab dimension. No bias (Mikolov-style)\n",
        "    self.linear_out = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    e = self.emb(x)\n",
        "    average = e.mean(dim=1)\n",
        "    out = self.linear_out(average)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "def one_hot_from_surroundings(surroundings, window_size, vocab_size):\n",
        "  \"\"\" Method to create one hot encodings for the surrounding words.\n",
        "  Arguments\n",
        "  ---------\n",
        "  surroundings : N x (Wx2) Tensor\n",
        "     Tensor with index of surrounding words, with N being the number of samples and W being the window size\n",
        "  vocab_size : int\n",
        "     Size of the vocabulary\n",
        "  Returns\n",
        "  -------\n",
        "  one_hot_surroundings : N x (Wx2) x V Tensor\n",
        "      Tensor with one hot encodings of surrounding words, with N being the number of samples, W being the window size and V being the vocabulary size\n",
        "  \"\"\"\n",
        "  one_hot_surr = []\n",
        "  for surr in surroundings:\n",
        "    surr_one_hot = []\n",
        "    for word in surr:\n",
        "      one_hot = torch.zeros(vocab_size, device=device)\n",
        "      one_hot[word] = 1\n",
        "      surr_one_hot.append(one_hot)\n",
        "    one_hot_surr.append(torch.stack(surr_one_hot))\n",
        "\n",
        "  return torch.stack(one_hot_surr)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #CAUTION: RUN THIS CODE WITH GPU, CPU WILL TAKE TOO LONG\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=5, lr=1e-3):\n",
        "    model.train()  # set to training mode\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for surr, tar in tqdm(train_loader, \"Training\"):\n",
        "\n",
        "        surr = surr.to(device)\n",
        "        tar  = tar.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(surr)\n",
        "        loss = loss_function(logits, tar)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()  # set to eval mode\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for surr, tar in tqdm(val_loader, \"Validation\"):\n",
        "            surr = surr.to(device)\n",
        "            tar  = tar.to(device)\n",
        "\n",
        "            logits = model(surr)\n",
        "            loss = loss_function(logits, tar)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ns-iKmJPdT9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING\n",
        "WINDOW_SIZES = [1, 2]\n",
        "EMBED_DIMS   = [100, 300]\n",
        "\n",
        "for WINDOW_SIZE in WINDOW_SIZES:\n",
        "    surroundings, targets = generate_dataset(dataset, WINDOW_SIZE, word_to_index)\n",
        "    print(f\"\\nWINDOW_SIZE = {WINDOW_SIZE}\")\n",
        "    print(surroundings.shape)\n",
        "    print(targets.shape)\n",
        "\n",
        "    full_dataset = list(zip(surroundings,targets))\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    train_dataloader=DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
        "    val_dataloader=DataLoader(val_dataset,batch_size=64,shuffle=False)\n",
        "\n",
        "    for EMB_DIM in EMBED_DIMS:\n",
        "        print(f\"\\n  -> EMB_DIM = {EMB_DIM}\")\n",
        "\n",
        "        CBOW_MODEL = CBOW(len(word_to_index), embed_dim= EMB_DIM).to(device)\n",
        "        optimizer = torch.optim.Adam(CBOW_MODEL.parameters(), lr=0.001)\n",
        "        EPOCHS = 5\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "          train_loss = train_model(CBOW_MODEL, train_dataloader, val_dataloader, epochs=1, lr=0.001)\n",
        "          val_loss = evaluate_model(CBOW_MODEL, val_dataloader)\n",
        "          print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        save_path = f\"/content/drive/MyDrive/Colab Notebooks/NLP-Assignment1/cbow_ws{WINDOW_SIZE}_emb{EMB_DIM}.pth\"\n",
        "        torch.save(CBOW_MODEL.state_dict(), save_path)\n",
        "        print(f\"Saved weights: {save_path}\")"
      ],
      "metadata": {
        "id": "LLdc1tMgdVqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "def create_word_to_embedding(vocab_list, model, word_to_index):\n",
        "    \"\"\"\n",
        "    Build {word: embedding_vector} using the CBOW's nn.Embedding weights.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "\n",
        "        W = model.emb.weight.detach().cpu()\n",
        "\n",
        "    word_to_embedding = {}\n",
        "    V = W.shape[0]\n",
        "    for word in vocab_list:\n",
        "        idx = word_to_index.get(word, word_to_index[\"OOV\"])\n",
        "        if 0 <= idx < V:\n",
        "            word_to_embedding[word] = W[idx]\n",
        "        else:\n",
        "            word_to_embedding[word] = W[word_to_index[\"OOV\"]]\n",
        "    return word_to_embedding\n",
        "\n",
        "\n",
        "from keras.datasets import imdb\n",
        "\n",
        "def decode_review(word_index, review):\n",
        "  decoded_review = \" \".join([reverse_word_index.get(i - 3, \"\") for i in review])\n",
        "  return decoded_review\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels)= imdb.load_data(num_words=10000)\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "# Trim for speed\n",
        "train_data = train_data[:10000]\n",
        "train_labels = train_labels[:10000]\n",
        "test_data = test_data[:2000]\n",
        "test_labels = test_labels[:2000]\n",
        "\n",
        "# Decode\n",
        "train_data = [decode_review(reverse_word_index,x) for x in train_data]\n",
        "test_data  = [decode_review(reverse_word_index,x) for x in test_data]\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_review_embedding(review, word_to_embedding):\n",
        "    words = review.lower().split()\n",
        "    valid = [word_to_embedding[w] for w in words if w in word_to_embedding]\n",
        "    if len(valid) == 0:\n",
        "        emb_dim = next(iter(word_to_embedding.values())).shape[0]\n",
        "        return torch.zeros(emb_dim)\n",
        "    return torch.stack(valid).mean(dim=0)\n",
        "\n",
        "def create_dataloader_imdb(train_data, train_labels, test_data, test_labels, word_to_embedding, batch_size=64):\n",
        "    train_dataset, test_dataset = [], []\n",
        "    for i, review in enumerate(train_data):\n",
        "        train_dataset.append((get_review_embedding(review, word_to_embedding),\n",
        "                              torch.tensor(train_labels[i], dtype=torch.long)))\n",
        "    for i, review in enumerate(test_data):\n",
        "        test_dataset.append((get_review_embedding(review, word_to_embedding),\n",
        "                             torch.tensor(test_labels[i], dtype=torch.long)))\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "# ---- Simple classifier\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim=128, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def train_sent(model, train_loader, optimizer, loss_function):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for X, y in tqdm(train_loader, \"Training (Sentiment)\"):\n",
        "        X = X.to(device).float()\n",
        "        y = y.to(device).long()\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = loss_function(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def eval_sent(model, val_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_predictions, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm(val_loader, \"Validation (Sentiment)\"):\n",
        "            X = X.to(device).float()\n",
        "            y = y.to(device).long()\n",
        "            logits = model(X)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            all_predictions.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(y.cpu().tolist())\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    return acc, all_predictions, all_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "L4C4KhuxdWpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics & Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, math\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/NLP-Assignment1\"\n",
        "\n",
        "WINDOW_SIZES = [1, 2]\n",
        "EMBED_DIMS   = [100, 300]\n",
        "\n",
        "results = []\n",
        "\n",
        "for WINDOW_SIZE in WINDOW_SIZES:\n",
        "    for EMB_DIM in EMBED_DIMS:\n",
        "        ckpt_path = f\"{SAVE_DIR}/cbow_ws{WINDOW_SIZE}_emb{EMB_DIM}.pth\"\n",
        "        if not os.path.exists(ckpt_path):\n",
        "            print(f\"[SKIP] Missing checkpoint: {ckpt_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n=== DOWNSTREAM EVAL â€” WINDOW_SIZE={WINDOW_SIZE}, EMB_DIM={EMB_DIM} ===\")\n",
        "\n",
        "        VOCAB_SIZE = len(word_to_index)\n",
        "        CBOW_MODEL = CBOW(VOCAB_SIZE, embed_dim=EMB_DIM).to(device)\n",
        "        CBOW_MODEL.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "        CBOW_MODEL.eval()\n",
        "\n",
        "        # Build word->embedding map from CBOW emb table\n",
        "        word_to_embedding = create_word_to_embedding(vocab_list, CBOW_MODEL, word_to_index)\n",
        "\n",
        "        # Build IMDB loaders with current embedding dim\n",
        "        train_dataloader_imd, test_dataloader_imd = create_dataloader_imdb(\n",
        "            train_data, train_labels, test_data, test_labels, word_to_embedding, batch_size=64\n",
        "        )\n",
        "\n",
        "        # Sentiment model for this embedding size\n",
        "        SENT_MODEL = SentimentClassifier(embedding_dim=EMB_DIM).to(device)\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(SENT_MODEL.parameters(), lr=0.001)\n",
        "\n",
        "        # Train a few epochs\n",
        "        EPOCHS = 5\n",
        "        for ep in range(EPOCHS):\n",
        "            tr_loss = train_sent(SENT_MODEL, train_dataloader_imd, optimizer, loss_function)\n",
        "            acc, preds, labs = eval_sent(SENT_MODEL, test_dataloader_imd)\n",
        "            print(f\"[{ep+1}/{EPOCHS}] loss={tr_loss:.4f}  acc={acc:.4f}\")\n",
        "\n",
        "        # Final evaluation + metrics\n",
        "        acc, preds, labs = eval_sent(SENT_MODEL, test_dataloader_imd)\n",
        "        prec = precision_score(labs, preds, average='macro')\n",
        "        rec  = recall_score(labs, preds, average='macro')\n",
        "        f1   = f1_score(labs, preds, average='macro')\n",
        "        cm   = confusion_matrix(labs, preds, labels=[0,1])\n",
        "\n",
        "        print(f\"Macro Precision: {prec:.4f}\")\n",
        "        print(f\"Macro Recall:    {rec:.4f}\")\n",
        "        print(f\"Macro F1 Score:  {f1:.4f}\")\n",
        "        print(f\"Accuracy:        {acc:.4f}\")\n",
        "\n",
        "        # Save everything needed for comparison\n",
        "        results.append({\n",
        "            \"window_size\": WINDOW_SIZE,\n",
        "            \"embed_dim\": EMB_DIM,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": prec,\n",
        "            \"recall_macro\": rec,\n",
        "            \"f1_macro\": f1,\n",
        "            \"checkpoint\": ckpt_path,\n",
        "            \"cm\": cm\n",
        "        })\n",
        "\n",
        "#  Summary table across hyperparameters\n",
        "if len(results) > 0:\n",
        "    df_results = pd.DataFrame([\n",
        "        {k: v for k, v in r.items() if k != \"cm\"} for r in results\n",
        "    ]).sort_values([\"window_size\",\"embed_dim\"]).reset_index(drop=True)\n",
        "\n",
        "    display(df_results)\n",
        "    out_csv = f\"{SAVE_DIR}/downstream_imdb_metrics.csv\"\n",
        "    df_results.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved summary metrics to: {out_csv}\")\n",
        "\n",
        "    # Confusion matrices in subplots for side-by-side comparison\n",
        "    n = len(results)\n",
        "    if n <= 2:\n",
        "        cols = n\n",
        "    elif n <= 4:\n",
        "        cols = 2\n",
        "    else:\n",
        "        cols = 3\n",
        "    rows = math.ceil(n / cols)\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
        "    if rows * cols == 1:\n",
        "        axes = np.array([axes])\n",
        "    axes = axes.reshape(rows, cols)\n",
        "\n",
        "    for i, res in enumerate(results):\n",
        "        r, c = divmod(i, cols)\n",
        "        ax = axes[r, c]\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=res[\"cm\"],\n",
        "                                      display_labels=[\"Negative\",\"Positive\"])\n",
        "        disp.plot(ax=ax, cmap=plt.cm.Blues, values_format='d', colorbar=False)\n",
        "        ax.set_title(f\"WS={res['window_size']}, D={res['embed_dim']}\\n\"\n",
        "                     f\"Acc={res['accuracy']:.3f} | F1={res['f1_macro']:.3f}\")\n",
        "        ax.grid(False)\n",
        "\n",
        "    for j in range(i+1, rows*cols):\n",
        "        axes.flat[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No results to summarize (no checkpoints found/evaluated).\")\n"
      ],
      "metadata": {
        "id": "Mg6rENBMdYNh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tMjs6zu63bnh",
        "2RtspVFxcHjb",
        "GdyG1oAccRFu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}